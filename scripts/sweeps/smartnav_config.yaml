---
trainer:
  steps: 300
  workers: 1

  tensorboard_name:
  save_freq: 10

  PPOConfig:
    optimizer: adam
    OptimizerKwargs:
      lr: 0.00005
      betas: !!python/tuple [0.9, 0.999]
      eps: 1.0e-07
      weight_decay: 0
      amsgrad: false

    gamma: 0.99
    eta: 0.0
    gae_lambda: 0.9

    advantage_normalization: true

    eps: 0.2
    target_kl: 0.03
    entropy_coeff: 0.0
    entropy_decay_time: 100
    min_entropy: 0.0
    value_coeff: 1.0

    ppo_epochs: 20
    minibatch_size: 6400

    use_gpu:

model:
  input_size:
  num_actions:
  discrete: false

  activation: leaky_relu

  hidden_sizes: [1024, 1024]

  sigma0: 1.0
  std_head: false

  initializer: orthogonal

environment:
  win_reward: 1.0
  lose_reward: -1.0
  timestep_reward: -0.0005
  closer_reward: 1.0
  visible_reward: -0.005
  win_threshold: 1.5
