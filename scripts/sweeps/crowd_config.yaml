---
trainer:
  steps: 500
  workers: 4

  tensorboard_name:
  save_freq: 10

  PPOConfig:
    optimizer: adam
    OptimizerKwargs:
      lr: 0.00005
      betas: !!python/tuple [0.9, 0.999]
      eps: 1.0e-07
      weight_decay: 0
      amsgrad: false

    gamma: 0.99
    eta: 0.0
    gae_lambda: 0.95

    advantage_normalization: true

    eps: 0.2
    target_kl: 0.03
    entropy_coeff: 0.0
    entropy_decay_time: 100
    min_entropy: 0.0
    value_coeff: 1.0

    ppo_epochs: 10
    minibatch_size: 2048

    use_gpu:

model:
  input_size:
  num_actions:
  rel_input_size:
  activation: tanh
  discrete: false

  hidden_sizes: [64, 64, 64]
  separate_value: true


  conv_filters: 2

  sigma0: 1.0

  vec_hidden_layers: [32, 32]
  rel_hidden_layers: [32, 32]
  com_hidden_layers: [32, 32]

  initializer: orthogonal

environment:
  agents: 20
  mode: random

  # Reward coefficients
  potential: 0.4
  goal: 1.0
  collision: -0.3
  sight_radius: 5.0
  sight_agents: 10.0

  comfort_speed: 1.4
  comfort_speed_weight: 1.5

  comfort_distance: 1.5
  comfort_distance_weight: 1.0