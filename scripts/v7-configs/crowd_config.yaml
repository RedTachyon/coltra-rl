---
trainer:
  steps: 200
  workers: 2

  tensorboard_name:
  save_freq: 100

  PPOConfig:
    optimizer: adam
    OptimizerKwargs:
      lr: 8.7e-05
      betas: !!python/tuple [0.9, 0.999]
      eps: 1.0e-07
      weight_decay: 0
      amsgrad: false

    gamma: 0.993
    eta: 0.0
    gae_lambda: 0.891

    advantage_normalization: true

    eps: 0.143
    target_kl: 0.018
    entropy_coeff: 0.0
    entropy_decay_time: 100
    min_entropy: 0.0
    value_coeff: 1.0

    ppo_epochs: 30
    minibatch_size: 2048

    use_gpu:

model:
  activation: tanh
  initializer: orthogonal

  sigma0: 0.4

  beta: false

  hidden_sizes: [64, 64, 64]
  separate_value: true


  vec_hidden_layers: [128, 128, 128]
  rel_hidden_layers: [128, 128, 128]
  com_hidden_layers: [128, 128]
  emb_size: 64
  attention_heads: 2


environment:
  num_agents: 50
  initializer: "Crossway"
  dynamics: "CartesianAcceleration"
  observer: "Relative"
  rewarder: "BaseRewarder"

  # Reward function
  potential: 1
  goal: 10
  collision: -0.05
  step_reward: -0.005

  standstill_weight: 0
  standstill_exponent: 0
  goal_speed_threshold: 0

  comfort_speed: 1.33
  comfort_speed_weight: -0.75
  comfort_speed_exponent: 1

  comfort_distance: 0
  comfort_distance_weight: 0

  energy_weight: 1
  final_energy_weight: 1

  # Observations
  sight_radius: 10
  sight_agents: 50
  sight_angle: 180
  sight_acceleration: false

  rays_per_direction: 10
  ray_length: 10
  ray_degrees: 90
  ray_agent_vision: true
  destroy_raycasts: true

  # Spawn
  spawn_noise_scale: 0.3
  spawn_scale: 5

  grid_spawn: true
  group_spawn_scale: 1.5

  enable_obstacles: true
  block_scale: 3

  random_mass: true
  random_energy: true

  shared_goal: true


  evaluation_mode: false
  # save_path:  # This should probably be set manually
  early_finish: false
  nice_colors: true

  show_attention: false
  backwards_allowed: true

model_type: "relation"
